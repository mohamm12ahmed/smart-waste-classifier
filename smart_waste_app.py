import streamlit as st
from PIL import Image
import torch
import torchvision.transforms as transforms
import torch.nn.functional as F
import cv2
import numpy as np
from model import loaded_model

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù‡Ø§Ø²
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
model = loaded_model.to(device)
model.load_state_dict(torch.load("waste_classifier.pt", map_location=device))
model.eval()

# Ø§Ù„ÙØ¦Ø§Øª
classes = ["Plastic", "Glass", "Paper", "Cardboard", "Metal", "Trash"]

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
])

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
st.title("ğŸ“± Smart Waste Classification System")
st.subheader("Choose an image input method:")

input_option = st.radio("Select input method:", ["ğŸ“· Capture with Camera", "ğŸ–¼ï¸ Upload from Device", "ğŸ¥ Live Video Detection"])

image = None

# ğŸ“· Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
if input_option == "ğŸ“· Capture with Camera":
    camera_image = st.camera_input("Take a picture")
    if camera_image is not None:
        image = Image.open(camera_image).convert("RGB")

# ğŸ–¼ï¸ Ø±ÙØ¹ ØµÙˆØ±Ø©
elif input_option == "ğŸ–¼ï¸ Upload from Device":
    uploaded_file = st.file_uploader("Upload an image", type=["jpg", "jpeg", "png"])
    if uploaded_file is not None:
        image = Image.open(uploaded_file).convert("RGB")

# ğŸ¥ ÙÙŠØ¯ÙŠÙˆ Ù…Ø¨Ø§Ø´Ø±
elif input_option == "ğŸ¥ Live Video Detection":
    stframe = st.empty()
    cap = cv2.VideoCapture(0)

    run = st.checkbox("Start Camera")

    while run and cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            st.error("Failed to grab frame.")
            break

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ RGB Ù„Ù€ PIL
        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(img).convert("RGB")

        # ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„ØªØµÙ†ÙŠÙ
        input_tensor = transform(pil_img).unsqueeze(0).to(device)

        with torch.no_grad():
            outputs = model(input_tensor)
            _, predicted = torch.max(outputs, 1)
            predicted_class = classes[predicted.item()]

        # ÙƒØªØ§Ø¨Ø© Ø§Ù„ØªØµÙ†ÙŠÙ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©
        cv2.putText(frame, f"Class: {predicted_class}", (10, 40),
                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)

        # Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© ÙÙŠ Streamlit
        stframe.image(frame, channels="BGR")

    cap.release()
    st.warning("Video stream ended.")

# ØªØµÙ†ÙŠÙ ØµÙˆØ±Ø© Ø¹Ø§Ø¯ÙŠØ©
if image is not None:
    st.image(image, caption="ğŸ“¸ Input Image", use_column_width=True)

    if st.button("Classify"):
        input_tensor = transform(image).unsqueeze(0).to(device)
        with torch.no_grad():
            outputs = model(input_tensor)
            _, predicted = torch.max(outputs, 1)
            predicted_class = classes[predicted.item()]

        st.success(f"âœ… Detected category: **{predicted_class}**")

        image_path = f"images/{predicted_class.lower()}.png"
        try:
            st.image(image_path, caption=f"ğŸ—‘ï¸ {predicted_class} bin", width=200)
        except:
            st.info("No illustrative image available for this category.")
